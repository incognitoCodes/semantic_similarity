{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f81d734",
   "metadata": {},
   "source": [
    "## Deploying a Hugging Face model in SageMaker for Semantic Similarity [Long Documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfe2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SageMaker -U --quiet\n",
    "! pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f8c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "runtime= boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68f7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This hugging face model is suitable for long documents \n",
    "hub = {\n",
    "    'HF_MODEL_ID':'allenai/longformer-base-4096',\n",
    "    'HF_TASK':'feature-extraction'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2e6f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.17.0',\n",
    "    pytorch_version='1.10.2',\n",
    "    py_version='py38',\n",
    "    env=hub,\n",
    "    role=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0911c",
   "metadata": {},
   "source": [
    "## Invoking a real time endpoint in SageMaker using boto3 SageMaker runtime\n",
    "\n",
    "Once the endpoint is created, you can find the endpoint name in the SageMaker console under \"Inference\" > \"endpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a76857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'huggingface-pytorch-inference-2023-01-21-11-14-27-348'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232c3085",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload1 = {\"inputs\": \"what is S3\"}\n",
    "payload2 = {\"inputs\": \"what does S3 do\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c99f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='application/json',\n",
    "                                       Body=json.dumps(payload1))\n",
    "sent1 = response1['Body'].read().decode()\n",
    "sent1_embedding = np.array(ast.literal_eval(sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae1290b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='application/json',\n",
    "                                       Body=json.dumps(payload2))\n",
    "sent2 = response2['Body'].read().decode()\n",
    "sent2_embedding = np.array(ast.literal_eval(sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9716f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def pad_to_length(x, arraysize):\n",
    "        return np.pad(x,((0, 0), (0, arraysize - x.shape[1])), mode = 'constant')\n",
    "    \n",
    "\n",
    "def cos_similarity_vectors_diff_size(vec1, vec2):\n",
    "    vec1_embed_np = vec1.reshape(1,-1)\n",
    "    vec2_embed_np = vec2.reshape(1,-1)\n",
    "\n",
    "    maxsize = max(i.shape[1] for i in [vec1_embed_np,vec2_embed_np])\n",
    "\n",
    "    padded_vec1 = pad_to_length(vec1_embed_np, maxsize)\n",
    "    padded_vec2 = pad_to_length(vec2_embed_np, maxsize)\n",
    "\n",
    "    return cosine_similarity(padded_vec1,padded_vec2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fbda950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8763233750811223"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity_vectors_diff_size(sent1_embedding,sent2_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30afdece",
   "metadata": {},
   "source": [
    "## Deploying serverless inference in SageMaker\n",
    "\n",
    "Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers. Serverless Inference integrates with AWS Lambda to offer you high availability, built-in fault tolerance and automatic scaling.\n",
    "\n",
    "With a pay-per-use model, Serverless Inference is a cost-effective option if you have an infrequent or unpredictable traffic pattern. During times when there are no requests, Serverless Inference scales your endpoint down to 0, helping you to minimize your costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2a7bc",
   "metadata": {},
   "source": [
    "### Endpoint Configuration Creation\n",
    "\n",
    "This is where you can adjust the Serverless Configuration for your endpoint. You will define the max concurrent invocations for a single endpoint, known as **MaxConcurrency**, and the **Memory size**. \n",
    "\n",
    "Your serverless endpoint has a minimum RAM size of 1024 MB (1 GB), and the maximum RAM size you can choose is 6144 MB (6 GB). **The memory sizes you can choose are 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, or 6144 MB**. Serverless Inference auto-assigns compute resources proportional to the memory you select. If you choose a larger memory size, your container has access to more vCPUs. \n",
    "\n",
    "Serverless endpoints have a quota for how many concurrent invocations can be processed at the same time. If the endpoint is invoked before it finishes processing the first request, then it handles the second request concurrently. **You can set the maximum concurrency for a single endpoint up to 200, and the total number of serverless endpoints you can host in a Region is 50**. The maximum concurrency for an individual endpoint prevents that endpoint from taking up all of the invocations allowed for your account, and any endpoint invocations beyond the maximum are throttled.\n",
    "\n",
    "If your endpoint does not receive traffic for a while and then your endpoint suddenly receives new requests, it can take some time for your endpoint to spin up the compute resources to process the requests. This is called a **cold start**. Since serverless endpoints provision compute resources on demand, your endpoint may experience cold starts. A cold start can also occur if your concurrent requests exceed the current concurrent request usage. The cold start time depends on your model size, how long it takes to download your model, and the start-up time of your container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a6637e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig\n",
    "from time import gmtime, strftime\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Define the model name, which you can find in SageMaker console and automatically created by the real time endpoint\n",
    "model_name='huggingface-pytorch-inference-2022-11-22-13-59-51-293'\n",
    "\n",
    "# Define the serverless configuration in terms of memory size and max concurrency\n",
    "rcf_serverless_config = 'hf-serverless-epc'+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=rcf_serverless_config,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"HFVariant\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"ServerlessConfig\": {\n",
    "                \"MemorySizeInMB\": 6144,\n",
    "                \"MaxConcurrency\": 1,\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Creating the serverless endpoint\n",
    "endpoint_name = \"HF-Serverless-semanticSimilarity-ep\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())    # give a suitable name\n",
    "\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=rcf_serverless_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf280a5",
   "metadata": {},
   "source": [
    "Now, let's check the endpoint creation status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f13a64fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:22:07 : Endpoint: Creating\n",
      "11:23:07 : Endpoint: Creating\n",
      "11:24:07 : Endpoint: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "max_time = time.time() + 15*60 # 15 min\n",
    "\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response['EndpointStatus']\n",
    "    print(f\"{current_time} : Endpoint: {status}\")\n",
    "\n",
    "    if status=='InService':\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac692b3",
   "metadata": {},
   "source": [
    "### Invoking the serverless endpoint using boto3 SageMaker runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a361859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FinancialAdvisor_JD.txt', 'r') as file:\n",
    "    JD = file.read().replace(\"\\n\", \"\").replace(\"\\ufeff\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e51d3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_resume.txt', 'r') as file:\n",
    "    finance_advisor_resume = file.read().replace(\"\\n\", \"\").replace(\"\\ufeff\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7308b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload1 = {\"inputs\": JD}\n",
    "payload2 = {\"inputs\": finance_advisor_resume}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45197483",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='application/json',\n",
    "                                       Body=json.dumps(payload1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbe07aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = response1['Body'].read().decode()\n",
    "sent1_embedding = np.array(ast.literal_eval(sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "449ee205",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='application/json',\n",
    "                                       Body=json.dumps(payload2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "937ebb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = response2['Body'].read().decode()\n",
    "sent2_embedding = np.array(ast.literal_eval(sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec4d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def pad_to_length(x, arraysize):\n",
    "        return np.pad(x,((0, 0), (0, arraysize - x.shape[1])), mode = 'constant')\n",
    "    \n",
    "\n",
    "def cos_similarity_vectors_diff_size(vec1, vec2):\n",
    "    vec1_embed_np = vec1.reshape(1,-1)\n",
    "    vec2_embed_np = vec2.reshape(1,-1)\n",
    "\n",
    "    maxsize = max(i.shape[1] for i in [vec1_embed_np,vec2_embed_np])\n",
    "\n",
    "    padded_vec1 = pad_to_length(vec1_embed_np, maxsize)\n",
    "    padded_vec2 = pad_to_length(vec2_embed_np, maxsize)\n",
    "\n",
    "    return cosine_similarity(padded_vec1,padded_vec2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2a4e0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16094102746989233"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity_vectors_diff_size(sent1_embedding, sent2_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8682d40",
   "metadata": {},
   "source": [
    "**Please note that this model supports word count of 4096 max for a document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2739f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
